"""测试LLM客户端模块"""import loggingimport sysfrom pathlib import Path# 添加项目根目录到路径sys.path.insert(0, str(Path(__file__).parent.parent))from src.llm_client import LLMClientdef test_llm_client():    """测试LLM客户端功能"""    logging.basicConfig(level=logging.INFO)    print("\n" + "="*50)    print("测试LLM客户端模块")    print("="*50)    llm = LLMClient()    # 1. 测试基本调用    print("\n=== 测试1: 基本LLM调用 ===")    response = llm.generate(        prompt="请用一句话介绍什么是RAG系统",        max_tokens=100    )    print(f"回复: {response}")    # 2. 测试流式输出    print("\n=== 测试2: 流式输出 ===")    print("回复: ", end="")    full_response = ""    for chunk in llm.generate_stream(        prompt="请解释什么是向量检索",        max_tokens=100    ):        print(chunk, end="", flush=True)        full_response += chunk    print()    print(f"\n完整回复长度: {len(full_response)} 字符")    # 3. 测试RAG场景    print("\n=== 测试3: RAG场景 ===")    mock_context = [        {            'rank': 1,            'qa_pair': {                'question': '如何注销账号？',                'answer': '进入设置-账号安全-注销账号，按照提示操作即可。注销后数据将被清除且无法恢复。'            },            'similarity': 0.95        },        {            'rank': 2,            'qa_pair': {                'question': '账号注销需要多久？',                'answer': '提交注销申请后，系统会在7个工作日内完成审核和注销。'            },            'similarity': 0.88        }    ]    response = llm.generate_with_context(        question="我想注销我的账号，需要多长时间？",        context=mock_context,        max_tokens=200    )    print(f"\n回复:\n{response}")    print("\n✅ LLM客户端测试完成！")if __name__ == "__main__":    test_llm_client()