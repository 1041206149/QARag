# 使用示例对比## 重构前后使用对比### 1. 初始化 RAG Pipeline#### ❌ 重构前```python# 需要手动指定所有参数pipeline = RAGPipeline(    data_path="data/raw/qa_pairs_rag.json",    embedding_model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",    top_k=3,    similarity_threshold=0.7,    use_cache=True)```#### ✅ 重构后```python# 所有参数自动从配置文件读取pipeline = RAGPipeline()# 或者只覆盖需要修改的参数pipeline = RAGPipeline(top_k=5)  # 其他参数仍使用配置文件```### 2. 配置日志系统#### ❌ 重构前```python# main.py 和 utils.py 中都有重复的日志配置代码import loggingfrom pathlib import Pathdef setup_logging():    log_dir = Path(__file__).parent / "logs"    log_dir.mkdir(parents=True, exist_ok=True)    log_file = log_dir / "app.log"        logging.basicConfig(        level=logging.INFO,        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',        handlers=[            logging.FileHandler(log_file, encoding='utf-8'),            logging.StreamHandler()        ]    )```#### ✅ 重构后```python# 统一使用配置管理器from config.config_manager import configconfig.setup_logging()  # 自动读取config.yaml中的日志配置```### 3. 获取配置值#### ❌ 重构前```python# 硬编码在各个类中class EmbeddingModel:    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):        self.model_name = model_name        # 路径计算也是硬编码        project_root = Path(__file__).resolve().parent.parent        self.cache_dir = project_root / "data" / "processed"```#### ✅ 重构后```python# 从配置管理器读取from config.config_manager import configclass EmbeddingModel:    def __init__(self, model_name: Optional[str] = None):        embedding_config = config.embedding_config        self.model_name = model_name or embedding_config.get('model_name')        # 路径由配置管理器统一处理        self.cache_dir = config.get_path('embedding.cache_dir')```### 4. LLM 客户端配置#### ❌ 重构前```python# 需要手动管理环境变量和默认值import osfrom dotenv import load_dotenvload_dotenv()class LLMClient:    def __init__(self, model=None, api_key=None, base_url=None, temperature=None):        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")        self.api_key = api_key or os.getenv("OPENAI_API_KEY")        self.base_url = base_url or os.getenv("OPENAI_API_BASE")        self.temperature = temperature if temperature is not None else float(os.getenv("OPENAI_TEMPERATURE", 1))# Prompt模板硬编码在代码中def _build_rag_prompt(self, question: str, context: str) -> str:    prompt = f"""请根据以下参考信息回答用户的问题。    === 参考信息 ==={context}..."""    return prompt```#### ✅ 重构后```python# 配置管理器自动整合环境变量和配置文件from config.config_manager import configfrom config.prompt_templates import RAG_PROMPT_TEMPLATEclass LLMClient:    def __init__(self, model=None, api_key=None, base_url=None, temperature=None):        llm_config = config.llm_config  # 自动整合环境变量        self.model = model or llm_config.get('model')        self.api_key = api_key or llm_config.get('api_key')        self.base_url = base_url or llm_config.get('base_url')        self.temperature = temperature if temperature is not None else llm_config.get('temperature', 1.0)# Prompt模板统一管理def _build_rag_prompt(self, question: str, context: str) -> str:    return RAG_PROMPT_TEMPLATE.format(context=context, question=question)```### 5. 修改配置参数#### ❌ 重构前需要修改多个文件中的硬编码值：```python# src/rag_pipeline.pytop_k: int = 3similarity_threshold: float = 0.7# src/embedding.pymodel_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"batch_size: int = 32# src/llm_client.pyos.getenv("OPENAI_MODEL", "gpt-4o-mini")```#### ✅ 重构后只需修改一个配置文件：```yaml# config/config.yamlretrieval:  top_k: 5  # 修改这里  similarity_threshold: 0.8  # 修改这里embedding:  model_name: "custom-model-name"  # 修改这里  batch_size: 64  # 修改这里llm:  model: "gpt-4"  # 修改这里```### 6. 环境切换#### ❌ 重构前```python# 需要修改代码pipeline = RAGPipeline(    data_path="data/test/qa_pairs.json",  # 测试环境    use_cache=False)```#### ✅ 重构后```bash# 方式1: 使用不同的配置文件cp config/config.test.yaml config/config.yaml# 方式2: 通过环境变量覆盖export OPENAI_MODEL=gpt-4export DATA_PATH=data/test/qa_pairs.json# 代码无需修改pipeline = RAGPipeline()```### 7. 获取项目路径#### ❌ 重构前```python# 每个模块都要计算project_root = Path(__file__).resolve().parent.parentdata_path = project_root / "data" / "raw" / "qa_pairs_rag.json"```#### ✅ 重构后```pythonfrom config.config_manager import config# 统一获取project_root = config.project_rootdata_path = config.get_path('data.raw_data_path')```## 配置优先级重构后支持多层配置覆盖：```代码参数 > 环境变量 > 配置文件 > 默认值```### 示例```python# config.yamlllm:  model: "gpt-4o-mini"  temperature: 1.0# .envOPENAI_MODEL=gpt-4OPENAI_TEMPERATURE=0.7# 代码client = LLMClient(temperature=0.5)# 最终生效的值：# model: "gpt-4"          (来自环境变量)# temperature: 0.5        (来自代码参数，优先级最高)```## 使用建议### 开发环境```python# 直接使用配置文件默认值，快速启动pipeline = RAGPipeline()```### 测试环境```python# 通过参数覆盖，便于测试pipeline = RAGPipeline(    top_k=1,    similarity_threshold=0.5,    use_cache=False)```### 生产环境```bash# 通过环境变量配置敏感信息export OPENAI_API_KEY=sk-xxxexport OPENAI_API_BASE=https://api.openai.com/v1# 代码保持简洁pipeline = RAGPipeline()```## 代码量对比| 文件 | 重构前 | 重构后 | 减少 ||------|--------|--------|------|| main.py | ~200行 | ~160行 | -20% || utils.py | ~80行 | ~60行 | -25% || 各模块路径处理 | 重复代码 | 统一调用 | -50% || **总计** | **更多重复** | **更简洁** | **~30%** |## 可维护性提升### 配置变更影响范围**重构前：**```修改top_k配置 → 需要修改3处代码 → 容易遗漏 → 可能产生不一致```**重构后：**```修改top_k配置 → 只需修改config.yaml → 全局生效 → 保证一致性```### 新增配置项**重构前：**```1. 在类中添加参数2. 在__init__中添加默认值3. 更新所有调用处4. 更新文档```**重构后：**```1. 在config.yaml中添加配置2. 代码自动读取（无需修改）```## 总结重构后的代码：- ✅ **更简洁**：减少30%的重复代码- ✅ **更灵活**：支持多层配置覆盖- ✅ **更易维护**：配置集中管理- ✅ **更易测试**：配置与业务解耦- ✅ **向后兼容**：保留原有调用方式